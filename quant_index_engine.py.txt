import streamlit as st
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler

st.set_page_config(layout="wide")
st.title("ðŸ“ˆ Institutional Index Event Engine")

# --------------------------------------------------
# Universe
# --------------------------------------------------

tickers = [
    "AAPL","MSFT","AMZN","NVDA","GOOGL","META","TSLA","BRK-B",
    "UNH","XOM","JPM","V","PG","MA","HD","CVX","LLY","MRK",
    "ABBV","PEP","AVGO","COST","KO","WMT","MCD","TMO",
    "ACN","DHR","ABT","LIN","ADBE","CRM","NFLX","AMD",
    "INTC","CMCSA","QCOM","TXN","NEE","RTX","HON","LOW",
    "UNP","ORCL","UPS","PM","IBM","AMGN","SPGI","INTU"
]

# --------------------------------------------------
# Load Data
# --------------------------------------------------

@st.cache_data
def load_data():

    data = yf.download(
        tickers + ["SPY"],
        start="2016-01-01",
        end="2025-01-01",
        auto_adjust=False,
        progress=False
    )

    prices = data["Adj Close"]
    volumes = data["Volume"]

    prices = prices.drop(columns=["SPY"])
    volumes = volumes.drop(columns=["SPY"])

    returns = prices.pct_change()
    volatility = returns.rolling(60).std() * np.sqrt(252)
    adv = (prices * volumes).rolling(60).mean()

    return prices, volatility, adv


# --------------------------------------------------
# Shares Outstanding
# --------------------------------------------------

@st.cache_data
def get_shares():

    shares = {}
    for t in tickers:
        try:
            shares[t] = yf.Ticker(t).info.get("sharesOutstanding", None)
        except:
            shares[t] = None

    return pd.Series(shares)


# --------------------------------------------------
# Rebalance Dates
# --------------------------------------------------

def get_rebalance_dates():

    dates = []

    for year in range(2017, 2025):
        for month in [3,6,9,12]:
            fridays = pd.date_range(
                start=f"{year}-{month}-01",
                end=f"{year}-{month}-28",
                freq="W-FRI"
            )
            if len(fridays) >= 3:
                dates.append(fridays[2])

    return pd.to_datetime(dates)


# --------------------------------------------------
# Backtest
# --------------------------------------------------

def run_backtest(prices, volatility, adv, shares):

    rebalance_dates = get_rebalance_dates()
    rolling_pnl = []

    for i in range(len(rebalance_dates)-1):

        date_t = rebalance_dates[i]
        date_t1 = rebalance_dates[i+1]

        if date_t not in prices.index or date_t1 not in prices.index:
            continue

        snap_t = pd.DataFrame({
            "price": prices.loc[date_t],
            "volatility": volatility.loc[date_t],
            "adv": adv.loc[date_t],
            "shares": shares
        }).dropna()

        snap_t["float_mcap"] = snap_t["price"] * snap_t["shares"] * 0.9
        snap_t = snap_t.sort_values("float_mcap", ascending=False)
        snap_t["rank"] = range(1, len(snap_t)+1)
        snap_t["rank_distance"] = snap_t["rank"] - 30

        snap_t1 = pd.DataFrame({
            "price": prices.loc[date_t1],
            "shares": shares
        }).dropna()

        snap_t1["float_mcap"] = snap_t1["price"] * snap_t1["shares"] * 0.9
        snap_t1 = snap_t1.sort_values("float_mcap", ascending=False)
        snap_t1["rank"] = range(1, len(snap_t1)+1)

        merged = snap_t.merge(
            snap_t1[["rank"]],
            left_index=True,
            right_index=True,
            how="left",
            suffixes=("_t","_t1")
        )

        merged["inclusion"] = (
            (merged["rank_t"] > 30) &
            (merged["rank_t1"] <= 30)
        )

        features = merged[["rank_distance","volatility","adv"]]
        target = merged["inclusion"]

        mask = features.notnull().all(axis=1)
        features = features[mask]
        target = target[mask]

        if target.sum() == 0:
            continue

        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(features)

        lr = LogisticRegression()
        gb = GradientBoostingClassifier()

        lr.fit(X_scaled, target)
        gb.fit(features, target)

        prob = (lr.predict_proba(X_scaled)[:,1] +
                gb.predict_proba(features)[:,1]) / 2

        merged = merged.loc[mask].copy()
        merged["prob"] = prob

        trade = merged[merged["rank_distance"].abs() <= 5].copy()

        if len(trade) == 0:
            continue

        trade["weight"] = trade["prob"] - 0.5

        if trade["weight"].abs().sum() == 0:
            continue

        trade["weight"] /= trade["weight"].abs().sum()
        trade["weight"] *= trade["adv"] / trade["adv"].max()

        future_prices = prices.shift(-20)
        fwd_ret = (future_prices / prices - 1)

        pnl = (fwd_ret.loc[date_t, trade.index] *
               trade["weight"]).sum()

        rolling_pnl.append(pnl)

    rolling_pnl = pd.Series(rolling_pnl)

    if len(rolling_pnl) > 3:
        sharpe = rolling_pnl.mean() / rolling_pnl.std() * np.sqrt(4)
        return rolling_pnl, sharpe

    return rolling_pnl, None


# --------------------------------------------------
# UI Button
# --------------------------------------------------

if st.button("Run Backtest"):

    st.write("Loading market data...")
    prices, volatility, adv = load_data()

    st.write("Loading shares outstanding...")
    shares = get_shares()

    st.write("Running model...")
    pnl, sharpe = run_backtest(prices, volatility, adv, shares)

    if sharpe:
        st.success(f"Quarterly Annualized Sharpe: {round(sharpe,2)}")
        st.line_chart(pnl.cumsum())
    else:
        st.warning("Not enough signals.")